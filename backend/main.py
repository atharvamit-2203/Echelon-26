from fastapi import FastAPI, File, UploadFile, Form, BackgroundTasks, Body, HTTPException, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from firebase_service import FirebaseService
from ats_analysis import ATSAnalysisService
from ml_fair_hire_sentinel import FairHireSentinel
import json
from datetime import datetime
import asyncio
import os
from dotenv import load_dotenv
from uuid import uuid4
from google.cloud.firestore_v1.base_query import FieldFilter

# Load environment variables
load_dotenv()

# Frontend origin from environment with sensible default
FRONTEND_ORIGIN = os.getenv("FRONTEND_ORIGIN", "http://localhost:3000")

app = FastAPI(title="Fair-Hire Sentinel API")

# Include v1 API router
try:
    from app.api.v1.api import api_router
    app.include_router(api_router, prefix="/api/v1")
    print("âœ“ Mounted /api/v1 routes")
except Exception as e:
    print(f"Warning: Could not mount v1 API: {e}")

# Initialize ML-powered Fair-Hire Sentinel with local ML models
try:
    print("Initializing Fair-Hire Sentinel with local ML models...")
    ml_sentinel = FairHireSentinel()
    print("âœ“ Fair-Hire Sentinel ready")
except Exception as e:
    print(f"Warning: Could not initialize ML Sentinel: {e}")
    ml_sentinel = None

app.add_middleware(
    CORSMiddleware,
    allow_origins=[FRONTEND_ORIGIN],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class MetricData(BaseModel):
    title: str
    value: str
    delta: str
    trend: str = "up"

class AlertData(BaseModel):
    type: str
    title: str
    description: str
    affected: str
    recommendation: str = ""

class CVData(BaseModel):
    name: str
    email: str
    phone: str
    age: int
    gender: str
    experience: int
    skills: List[str]
    education: str
    location: str
    currentRole: str
    expectedSalary: str

class CandidateData(BaseModel):
    id: int
    age_group: str
    gender: str
    keywords: str
    score: int

class HomePageData(BaseModel):
    metrics: List[MetricData]
    alerts: List[AlertData]
    rescued_candidates: List[CandidateData]
    age_stats: dict
    gender_stats: dict

# ==== New Models for Companies/Roles/Policies ====
class CompanyCreate(BaseModel):
    name: str
    industry: str
    locations: Optional[List[str]] = []

class PolicyRules(BaseModel):
    hard_constraints: Dict[str, Any] = {}
    soft_constraints: Dict[str, Any] = {}

class RoleCreate(BaseModel):
    title: str
    family: Optional[str] = None
    required_skills: List[str] = []
    optional_skills: List[str] = []
    required_licenses: List[str] = []
    required_certifications: List[str] = []
    min_experience: Optional[int] = None
    max_experience: Optional[int] = None
    location: Optional[str] = None

class MatchRequest(BaseModel):
    candidateId: Optional[str] = None
    # Optional: allow direct CV payload
    cv: Optional[Dict[str, Any]] = None

@app.get("/")
def read_root():
    return {"message": "Fair-Hire Sentinel API"}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "fair-hire-sentinel-backend",
        "version": "1.0.0"
    }

@app.get("/api/home", response_model=HomePageData)
def get_home_data():
    # Try to get data from Firebase, fallback to static data
    try:
        firebase_metrics = FirebaseService.get_metrics()
        firebase_alerts = FirebaseService.get_alerts()
        firebase_candidates = FirebaseService.get_rescued_candidates()
        firebase_analytics = FirebaseService.get_analytics()
        
        if firebase_metrics and firebase_alerts:
            return HomePageData(
                metrics=[
                    MetricData(title="Total Candidates", value=str(firebase_metrics['totalCandidates']['value']), delta=firebase_metrics['totalCandidates']['delta']),
                    MetricData(title="ATS Rejections", value=str(firebase_metrics['atsRejections']['value']), delta=firebase_metrics['atsRejections']['delta'], trend="down"),
                    MetricData(title="Rescued Candidates", value=str(firebase_metrics['rescuedCandidates']['value']), delta=firebase_metrics['rescuedCandidates']['delta']),
                    MetricData(title="Active Bias Alerts", value=str(firebase_metrics['activeBiasAlerts']['value']), delta=firebase_metrics['activeBiasAlerts']['delta'])
                ],
                alerts=[AlertData(**alert) for alert in firebase_alerts],
                rescued_candidates=[CandidateData(id=c['id'], age_group=c['ageGroup'], gender=c['gender'], keywords=c['keywords'], score=c['score']) for c in firebase_candidates],
                age_stats=firebase_analytics['ageStats'] if firebase_analytics else {},
                gender_stats=firebase_analytics['genderStats'] if firebase_analytics else {}
            )
    except Exception as e:
        print(f"Firebase error: {e}")
    
    # Generate dynamic alerts based on actual ML analysis
    try:
        # Get all CVs from Firestore only; avoid expensive file re-processing on each poll.
        all_cvs = FirebaseService.get_all_cvs()
        
        # Calculate real metrics
        total_candidates = len(all_cvs)
        rejected_count = len([cv for cv in all_cvs if cv.get('status') == 'rejected'])
        shortlisted_count = len([cv for cv in all_cvs if cv.get('status') in ['shortlisted', 'under_review']])
        
        # Run quick bias analysis
        bias_detected = False
        bias_score = 0.0
        affected_candidates = 0
        bias_keywords = []
        
        if ml_sentinel and total_candidates > 0:
            # Check for age-based bias
            older_candidates = [cv for cv in all_cvs if cv.get('age', 0) > 45 and cv.get('experience', 0) > 10]
            older_rejected = len([cv for cv in older_candidates if cv.get('status') == 'rejected'])
            
            if len(older_candidates) > 0:
                older_rejection_rate = older_rejected / len(older_candidates)
                overall_rejection_rate = rejected_count / total_candidates if total_candidates > 0 else 0
                
                # If older candidates have >25% higher rejection rate, flag bias
                if older_rejection_rate > overall_rejection_rate + 0.25:
                    bias_detected = True
                    bias_score = older_rejection_rate - overall_rejection_rate
                    affected_candidates = older_rejected
                    bias_keywords = ['age_discrimination', 'experience_bias']
        
        # Build alerts based on real data
        alerts = []
        if bias_detected and affected_candidates > 0:
            alerts.append(AlertData(
                type="warning",
                title="ðŸŸ¡ Age-Based Bias Detected in Hiring Process",
                description=f"Candidates over 45 with 10+ years experience show {int(bias_score*100)}% higher rejection rate than average.",
                affected=f"{affected_candidates} highly experienced candidates (age 45+, 10+ years exp)",
                recommendation="Review rejection criteria - high experience should be valued, not penalized"
            ))
        
        # Check for semantic mismatch bias
        semantic_issues = 0
        for cv in all_cvs[:20]:  # Sample check
            if cv.get('status') == 'rejected':
                skills = cv.get('skills', [])
                if len(skills) > 5 and cv.get('experience', 0) > 5:
                    semantic_issues += 1
        
        if semantic_issues > 3:
            alerts.append(AlertData(
                type="info",
                title="ðŸ¦¸ Talent Rescue Opportunity",
                description=f"{semantic_issues} qualified candidates rejected despite strong skills and experience.",
                affected=f"Candidates with 5+ skills and 5+ years experience"
            ))
        
        # Build dynamic metrics
        rejection_rate = int((rejected_count / total_candidates * 100)) if total_candidates > 0 else 0
        
        return HomePageData(
            metrics=[
                MetricData(title="Total Candidates", value=str(total_candidates), delta="+0"),
                MetricData(title="ATS Rejections", value=str(rejected_count), delta=f"{rejection_rate}%", trend="down" if rejection_rate < 40 else "up"),
                MetricData(title="Rescued Candidates", value=str(semantic_issues), delta=f"+{semantic_issues}"),
                MetricData(title="Active Bias Alerts", value=str(len(alerts)), delta="âš ï¸" if len(alerts) > 0 else "âœ“")
            ],
            alerts=alerts if alerts else [
                AlertData(
                    type="info",
                    title="âœ… No Critical Bias Detected",
                    description=f"Analysis of {total_candidates} candidates shows fair hiring patterns.",
                    affected="System monitoring active"
                )
            ],
            rescued_candidates=[
                CandidateData(
                    id=int(cv.get('candidateId', 'FILE_001').split('_')[-1]) if cv.get('candidateId') else idx,
                    age_group=">45" if cv.get('age', 30) > 45 else "30-45" if cv.get('age', 30) >= 30 else "<30",
                    gender=cv.get('gender', 'Unknown'),
                    keywords=', '.join(cv.get('skills', [])[:2]) if cv.get('skills') else 'N/A',
                    score=85 + (cv.get('experience', 0) * 2)
                )
                for idx, cv in enumerate([c for c in all_cvs if c.get('status') == 'rejected' and c.get('experience', 0) > 5][:3])
            ],
            age_stats={
                "Under 30": len([cv for cv in all_cvs if cv.get('age', 30) < 30]),
                "30-45": len([cv for cv in all_cvs if 30 <= cv.get('age', 30) <= 45]),
                "Over 45": len([cv for cv in all_cvs if cv.get('age', 30) > 45])
            },
            gender_stats={
                "Male": int(rejection_rate * 0.28) if total_candidates > 0 else 28,
                "Female": int(rejection_rate * 0.42) if total_candidates > 0 else 42,
                "Non-binary": int(rejection_rate * 0.30) if total_candidates > 0 else 30
            }
        )
    except Exception as e:
        print(f"Error generating dynamic alerts: {e}")
        import traceback
        traceback.print_exc()
    
    # Fallback to minimal data if error
    return HomePageData(
        metrics=[
            MetricData(title="Total Candidates", value="0", delta="--"),
            MetricData(title="ATS Rejections", value="0", delta="--"),
            MetricData(title="Rescued Candidates", value="0", delta="--"),
            MetricData(title="Active Bias Alerts", value="0", delta="âœ“")
        ],
        alerts=[
            AlertData(
                type="info",
                title="ðŸ“Š System Initializing",
                description="Loading candidate data and running bias analysis...",
                affected="Please wait"
            )
        ],
        rescued_candidates=[],
        age_stats={"Under 30": 0, "30-45": 0, "Over 45": 0},
        gender_stats={"Male": 0, "Female": 0, "Non-binary": 0}
    )

@app.post("/api/bulk-upload-cvs")
async def bulk_upload_cvs(cvs: List[dict]):
    try:
        results = []
        for cv_data in cvs:
            candidate_id = f"CV{datetime.now().strftime('%Y%m%d%H%M%S')}{len(results)}"
            cv_data['candidateId'] = candidate_id
            cv_data['uploadedAt'] = datetime.now()
            cv_data['status'] = 'under_review'
            
            FirebaseService.add_cv(cv_data)
            results.append(candidate_id)
        
        return {"message": f"Successfully uploaded {len(results)} CVs", "candidateIds": results}
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/upload-cv-file")
async def upload_cv_file(
    file: UploadFile = File(...),
    name: str = Form(...),
    email: str = Form(...),
    phone: str = Form(...),
    age: int = Form(...),
    gender: str = Form(...),
    experience: int = Form(...),
    education: str = Form(...),
    location: str = Form(...),
    currentRole: str = Form(...),
    expectedSalary: str = Form(...)
):
    try:
        # Generate candidate ID
        candidate_id = f"CV{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Read file contents
        contents = await file.read()
        
        # Parse CV content based on file type
        extracted_text = ""
        extracted_skills = []
        
        try:
            if file.filename.lower().endswith('.pdf'):
                import PyPDF2
                from io import BytesIO
                pdf_reader = PyPDF2.PdfReader(BytesIO(contents))
                for page in pdf_reader.pages:
                    extracted_text += page.extract_text() + "\n"
            
            elif file.filename.lower().endswith('.docx'):
                import docx
                from io import BytesIO
                doc = docx.Document(BytesIO(contents))
                for paragraph in doc.paragraphs:
                    extracted_text += paragraph.text + "\n"
            
            elif file.filename.lower().endswith('.txt'):
                extracted_text = contents.decode('utf-8')
            
            else:
                # Try to decode as text
                try:
                    extracted_text = contents.decode('utf-8')
                except:
                    extracted_text = "Could not extract text from file"
            
            # Extract skills using simple keyword matching
            if extracted_text and ml_sentinel:
                try:
                    # Use ML to extract skills from text
                    skill_keywords = ['python', 'javascript', 'react', 'node', 'sql', 'aws', 'docker', 'kubernetes', 'git', 'agile', 'scrum', 'java', 'c++', 'html', 'css', 'mongodb', 'postgresql', 'redis', 'tensorflow', 'pytorch', 'machine learning', 'data science', 'api', 'rest', 'graphql', 'microservices', 'devops', 'ci/cd', 'jenkins', 'terraform', 'ansible']
                    text_lower = extracted_text.lower()
                    extracted_skills = [skill for skill in skill_keywords if skill in text_lower]
                    
                    # Limit to top 10 skills
                    extracted_skills = extracted_skills[:10]
                except Exception as e:
                    print(f"Error extracting skills: {e}")
                    extracted_skills = ["To be updated"]
            
            if not extracted_skills:
                extracted_skills = ["To be updated"]
                
        except Exception as e:
            print(f"Error parsing CV file: {e}")
            extracted_text = "Error parsing file content"
            extracted_skills = ["To be updated"]
        
        # Save file to Firebase Storage (optional)
        file_url = None
        try:
            blob = FirebaseService.bucket.blob(f"cvs/{file.filename}")
            blob.upload_from_string(contents, content_type=file.content_type)
            file_url = blob.public_url
        except Exception as e:
            print(f"Warning: Could not save file to Firebase Storage: {e}")
        
        # Create CV data with extracted information
        cv_data = {
            "candidateId": candidate_id,
            "name": name,
            "email": email,
            "phone": phone,
            "age": age,
            "gender": gender,
            "experience": experience,
            "skills": extracted_skills,
            "education": education,
            "location": location,
            "currentRole": currentRole,
            "expectedSalary": expectedSalary,
            "fileName": file.filename,
            "fileUrl": file_url,
            "extractedText": extracted_text[:1000],  # Store first 1000 chars
            "status": "under_review",
            "uploadedAt": datetime.now()
        }
        
        # Save to Firebase
        FirebaseService.add_cv(cv_data)
        
        return {
            "message": "CV uploaded and parsed successfully",
            "candidateId": candidate_id,
            "cv": cv_data,
            "file_info": {
                "filename": file.filename,
                "file_size": len(contents),
                "extracted_skills": extracted_skills,
                "text_preview": extracted_text[:200] + "..." if len(extracted_text) > 200 else extracted_text
            }
        }
    except Exception as e:
        print(f"Error uploading CV: {e}")
        return {"error": str(e)}

@app.post("/api/upload-reference-cv")
async def upload_reference_cv(
    file: UploadFile = File(...),
    jobTitle: str = Form(...)
):
    try:
        # Generate reference CV ID
        ref_id = f"REF{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Read and extract text from file
        contents = await file.read()
        extracted_text = ""
        
        try:
            if file.filename.lower().endswith('.pdf'):
                import PyPDF2
                from io import BytesIO
                pdf_reader = PyPDF2.PdfReader(BytesIO(contents))
                for page in pdf_reader.pages:
                    extracted_text += page.extract_text() + "\n"
            elif file.filename.lower().endswith('.docx'):
                import docx
                from io import BytesIO
                doc = docx.Document(BytesIO(contents))
                for paragraph in doc.paragraphs:
                    extracted_text += paragraph.text + "\n"
            elif file.filename.lower().endswith('.txt'):
                extracted_text = contents.decode('utf-8')
        except Exception as e:
            print(f"Error extracting text: {e}")
            extracted_text = "Could not extract text"
        
        # Save file to Firebase Storage
        file_url = None
        try:
            blob = FirebaseService.bucket.blob(f"reference_cvs/{file.filename}")
            blob.upload_from_string(contents, content_type=file.content_type)
            file_url = blob.public_url
        except Exception as e:
            print(f"Warning: Could not save reference CV to Firebase Storage: {e}")
        
        # Allow multiple active reference CVs - no auto-deactivation
        
        # Save reference CV data
        ref_data = {
            "referenceId": ref_id,
            "jobTitle": jobTitle,
            "fileName": file.filename,
            "fileUrl": file_url,
            "extractedText": extracted_text,
            "uploadedAt": datetime.now(),
            "status": "active"
        }
        
        # Save to Firebase
        FirebaseService.db.collection('reference_cvs').document(ref_id).set(ref_data)
        
        return {
            "message": "Reference CV uploaded successfully. All future analyses will use this as reference.",
            "referenceId": ref_id,
            "jobTitle": jobTitle,
            "filename": file.filename
        }
    except Exception as e:
        print(f"Error uploading reference CV: {e}")
        return {"error": str(e)}

@app.post("/api/cvs")
async def add_cv(
    name: str = Form(...),
    email: str = Form(...),
    phone: str = Form(...),
    age: int = Form(...),
    gender: str = Form(...),
    experience: int = Form(...),
    skills: str = Form(...),
    education: str = Form(...),
    location: str = Form(...),
    currentRole: str = Form(...),
    expectedSalary: str = Form(...),
    cv_file: Optional[UploadFile] = File(None)
):
    try:
        # Generate candidate ID
        candidate_id = f"CV{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Parse skills from comma-separated string
        skills_list = [skill.strip() for skill in skills.split(',')]
        
        cv_data = {
            "candidateId": candidate_id,
            "name": name,
            "email": email,
            "phone": phone,
            "age": age,
            "gender": gender,
            "experience": experience,
            "skills": skills_list,
            "education": education,
            "location": location,
            "currentRole": currentRole,
            "expectedSalary": expectedSalary,
            "status": "under_review",
            "uploadedAt": datetime.now()
        }
        
        # Add CV to Firebase
        result = FirebaseService.add_cv(cv_data)
        return {"message": "CV added successfully", "candidateId": candidate_id}
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/cvs")
def get_cvs():
    try:
        # Get CVs ONLY from Firestore (user submissions)
        firestore_cvs = FirebaseService.get_all_cvs()
        
        return {"cvs": firestore_cvs, "total": len(firestore_cvs)}
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/cv-file-stats")
def get_cv_file_stats():
    try:
        stats = FirebaseService.get_cv_file_stats()
        return stats
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/recruiting-managers")
def get_recruiting_managers():
    try:
        managers = FirebaseService.get_recruiting_managers()
        return {"managers": managers}
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/companies")
def get_companies():
    try:
        companies = FirebaseService.get_companies()
        return {"companies": companies}
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/job-postings")
def get_job_postings():
    try:
        jobs_ref = FirebaseService.db.collection('job_postings').where(filter=FieldFilter('status', '==', 'active'))
        jobs = [job.to_dict() for job in jobs_ref.stream()]
        return {"jobs": jobs}
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/company-criteria/{company_name}")
def get_company_criteria(company_name: str):
    try:
        from company_ats_criteria import CompanyATSCriteria
        criteria = CompanyATSCriteria.get_company_criteria(company_name)
        return {"company": company_name, "criteria": criteria}
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/start-batch-analysis")
async def start_batch_analysis(background_tasks: BackgroundTasks):
    try:
        # Run ML-powered analysis in background
        background_tasks.add_task(run_ml_analysis)
        return {
            "message": "ML-powered Fair-Hire Sentinel analysis started",
            "status": "processing",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {"error": str(e)}

async def run_ml_analysis():
    """Run ML-powered bias detection and semantic analysis with two-stage screening"""
    try:
        # Get all CVs from Firestore only
        all_cvs = FirebaseService.get_all_cvs()
        
        if not all_cvs:
            print("No CVs found for analysis")
            return
        
        # Get job criteria (prefer reference CV context, then active saved criteria, then AI fallback).
        job_keywords = []
        use_multi_job = False
        
        try:
            # Check for ALL active reference CVs (supports multiple)
            ref_cv_ref = FirebaseService.db.collection('reference_cvs').where(
                filter=FieldFilter('status', '==', 'active')
            ).order_by('uploadedAt', direction='DESCENDING')
            ref_cv_docs = list(ref_cv_ref.stream())
            
            if ref_cv_docs and ml_sentinel:
                keyword_pool = []
                matched_families = []
                all_job_titles = []

                for ref_doc in ref_cv_docs:
                    ref_cv_data = ref_doc.to_dict()
                    ref_text = (ref_cv_data.get('extractedText') or '').strip()
                    ref_title = (ref_cv_data.get('jobTitle') or '').strip()

                    if ref_title:
                        all_job_titles.append(ref_title)
                        keyword_pool.extend(ml_sentinel.extract_required_skills(ref_title))

                    if ref_text:
                        # Use actual reference CV content to infer the closest job family keywords.
                        family_result = ml_sentinel.analyze_cv_against_job_families(ref_text)
                        best_match = family_result.get('best_match')
                        if best_match:
                            family_name = best_match[0]
                            matched_families.append(family_name)
                            keyword_pool.extend(ml_sentinel.JOB_FAMILIES[family_name]['keywords'])

                # Remove duplicates while preserving order.
                job_keywords = list(dict.fromkeys(keyword_pool))

                if job_keywords:
                    print(f"Using {len(ref_cv_docs)} active reference CV(s) for analysis context")
                    if all_job_titles:
                        print(f"Reference titles: {all_job_titles}")
                    if matched_families:
                        print(f"Reference-matched job families: {list(dict.fromkeys(matched_families))}")
                    print(f"Total derived keywords: {len(job_keywords)}")
            
            # If no reference CV context, use active recruiter-defined criteria.
            if not job_keywords:
                criteria_docs = list(
                    FirebaseService.db.collection('job_criteria')
                    .where(filter=FieldFilter('status', '==', 'active'))
                    .order_by('created_at', direction='DESCENDING')
                    .limit(1)
                    .stream()
                )
                if criteria_docs:
                    latest_criteria = criteria_docs[0].to_dict()
                    saved_keywords = latest_criteria.get('keywords') or []
                    if saved_keywords:
                        job_keywords = list(dict.fromkeys(saved_keywords))
                        print(
                            f"Using active job criteria: {latest_criteria.get('job_title', 'Unknown')} "
                            f"({len(job_keywords)} keywords)"
                        )

            # If still no criteria, use AI to analyze each CV individually.
            if not job_keywords:
                print("No usable reference CV context found - using AI to match CVs to best job families")
                use_multi_job = True
        except Exception as e:
            print(f"Error fetching reference CVs: {e} - using AI job family matching")
            use_multi_job = True
        
        print(f"Running analysis on {len(all_cvs)} CVs")
        if not use_multi_job and job_keywords:
            print(f"Job keywords: {job_keywords}")
        else:
            print("AI Mode: Using semantic analysis to match CVs to best positions across all industries")
        
        # Run ML analysis with two-stage screening
        analysis_results = ml_sentinel.run_full_analysis(all_cvs, job_keywords if not use_multi_job else [])
        
        # Update ALL CV statuses in Firebase with analysis results
        print(f"Updating CV statuses in Firebase...")
        
        # Update immediate interview candidates
        for cv in analysis_results.get('immediate_interviews', []):
            try:
                doc_id = cv.get('candidateId')
                if not doc_id:
                    # Try to find by name and userId
                    name = cv.get('name')
                    user_id = cv.get('userId')
                    if name:
                        docs = FirebaseService.db.collection('cvs').where(
                            filter=FieldFilter('name', '==', name)
                        ).limit(1).stream()
                        for doc in docs:
                            doc_id = doc.id
                            break
                
                if doc_id:
                    update_data = {
                        'status': 'selected',
                        'matchRate': cv.get('match_rate', 0),
                        'atsScore': cv.get('ats_score', 0),
                        'match_rate': cv.get('match_rate', 0),
                        'matched_keywords': cv.get('matched_keywords', 0),
                        'bestJobFamily': cv.get('best_job_family'),
                        'jobFamilyMatchScore': cv.get('job_family_match_score'),
                        'jobCategory': cv.get('job_category'),
                        'suggestedPosition': cv.get('best_job_family'),  # AI suggested position
                        'top3JobMatches': cv.get('top_3_job_matches', []),
                        'analysisDate': datetime.now(),
                        'analyzed': True
                    }
                    FirebaseService.db.collection('cvs').document(doc_id).set(update_data, merge=True)
                    position_info = f" (Suggested: {cv.get('best_job_family')})" if use_multi_job else ""
                    print(f"âœ“ Updated {cv.get('name')} - Selected ({cv.get('match_rate', 0):.0%} match){position_info}")
            except Exception as e:
                print(f"Error updating CV: {e}")
        
        # Update rescued candidates
        for alert in analysis_results.get('rescue_alerts', []):
            try:
                doc_id = alert.get('candidate_id')
                if not doc_id:
                    name = alert.get('name')
                    if name:
                        docs = FirebaseService.db.collection('cvs').where(
                            filter=FieldFilter('name', '==', name)
                        ).limit(1).stream()
                        for doc in docs:
                            doc_id = doc.id
                            break
                
                if doc_id:
                    update_data = {
                        'status': 'rescued',
                        'semanticScore': alert.get('semantic_score', 0),
                        'atsScore': alert.get('ats_score', 0),
                        'rescue_reason': alert.get('rescue_reason'),
                        'suggestedPosition': alert.get('best_job_family'),  # AI suggested position
                        'actualPotential': alert.get('actual_potential', 0),
                        'codingScore': alert.get('coding_score', 0),
                        'driftScore': alert.get('drift_score', 0),
                        'matchedKeywords': alert.get('matched_keywords', 0),
                        'totalKeywords': alert.get('total_keywords', 0),
                        'analysisDate': datetime.now(),
                        'analyzed': True
                    }
                    FirebaseService.db.collection('cvs').document(doc_id).set(update_data, merge=True)
                    position_info = f" (Suggested: {alert.get('best_job_family')})" if use_multi_job else ""
                    print(f"âœ“ Rescued {alert.get('name')} - {alert.get('semantic_score', 0):.0%} semantic match{position_info}")
            except Exception as e:
                print(f"Error updating rescued CV: {e}")
        
        # Update rejected candidates
        for cv in analysis_results.get('rejected', []):
            try:
                doc_id = cv.get('candidateId')
                if not doc_id:
                    name = cv.get('name')
                    if name:
                        docs = FirebaseService.db.collection('cvs').where(
                            filter=FieldFilter('name', '==', name)
                        ).limit(1).stream()
                        for doc in docs:
                            doc_id = doc.id
                            break
                
                if doc_id:
                    update_data = {
                        'status': 'rejected',
                        'rejection_reason': cv.get('rejection_reason'),
                        'atsScore': cv.get('ats_score', 0),
                        'match_rate': cv.get('match_rate', 0),
                        'semantic_analysis': cv.get('semantic_analysis'),
                        'analysisDate': datetime.now(),
                        'analyzed': True
                    }
                    FirebaseService.db.collection('cvs').document(doc_id).set(update_data, merge=True)
                    print(f"âœ— Rejected {cv.get('name')} - {cv.get('match_rate', 0):.0%} match")
            except Exception as e:
                print(f"Error updating rejected CV: {e}")
        
        # Save rescue alerts
        if analysis_results.get('rescue_alerts'):
            for alert in analysis_results['rescue_alerts']:
                FirebaseService.db.collection('alerts').add({
                    'type': 'rescue_alert',
                    'title': f'ðŸš¨ Qualified Candidate Rescued',
                    'description': f'{alert["name"]} has {alert["semantic_score"]:.0%} semantic match despite {alert.get("ats_score", 0):.0f}% keyword match',
                    'candidate_id': alert.get('candidate_id'),
                    'candidates': [alert],
                    'active': True,
                    'created_at': datetime.now(),
                    'severity': 'high'
                })
        
        # Save peer comparison bias alerts (similar CVs with different outcomes)
        bias_analysis = analysis_results.get('bias_analysis', {})
        peer_comparison_cases = bias_analysis.get('peer_comparison', [])
        
        if peer_comparison_cases:
            # Group all peer comparison cases into one alert
            FirebaseService.db.collection('alerts').add({
                'type': 'peer_comparison_bias',
                'title': f'âš ï¸ Disparate Treatment Detected: Similar Candidates, Different Outcomes',
                'description': f'Found {len(peer_comparison_cases)} case(s) where candidates with similar qualifications received different screening outcomes',
                'peer_cases': peer_comparison_cases,
                'candidates': [
                    {
                        'name': case['candidate_1']['name'],
                        'status': case['candidate_1']['status'],
                        'ats_score': case['candidate_1']['ats_score'],
                        'compared_with': case['candidate_2']['name']
                    }
                    for case in peer_comparison_cases
                ],
                'active': True,
                'created_at': datetime.now(),
                'severity': 'critical'
            })
            print(f"Peer comparison: Detected {len(peer_comparison_cases)} disparate treatment cases")
        
        # Update metrics
        FirebaseService.db.collection('metrics').document('current').set({
            'totalCandidates': {'value': len(all_cvs), 'delta': '+12%'},
            'atsRejections': {'value': len(analysis_results.get('rejected', [])), 'delta': '-8%'},
            'rescuedCandidates': {'value': len(analysis_results.get('rescue_alerts', [])), 'delta': '+15%'},
            'activeBiasAlerts': {'value': len(analysis_results.get('rescue_alerts', [])), 'delta': '+3%'},
            'lastUpdated': datetime.now()
        })
        
        print(f"ML Analysis completed: {len(analysis_results.get('rescue_alerts', []))} candidates rescued")
        
    except Exception as e:
        print(f"ML Analysis error: {e}")

@app.get("/api/analysis-status")
def get_analysis_status():
    try:
        # Get latest metrics to show current status
        metrics = FirebaseService.get_metrics()
        alerts = FirebaseService.get_alerts()
        return {
            "status": "completed" if metrics else "idle",
            "metrics": metrics,
            "active_alerts": len(alerts) if alerts else 0,
            "last_updated": metrics.get('lastUpdated') if metrics else None
        }
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/rescue-alerts")
def get_rescue_alerts():
    try:
        alerts_ref = FirebaseService.db.collection('alerts').where(
            filter=FieldFilter('type', '==', 'rescue_alert')
        ).where(
            filter=FieldFilter('active', '==', True)
        )
        alerts = [alert.to_dict() for alert in alerts_ref.stream()]
        return {"rescue_alerts": alerts}
    except Exception as e:
        return {"error": str(e)}

@app.get("/api/active-job-criteria")
def get_active_job_criteria():
    """Get active job criteria"""
    try:
        # Try to get from Firebase first
        criteria_ref = FirebaseService.db.collection('job_criteria').where(
            filter=FieldFilter('status', '==', 'active')
        ).order_by('created_at', direction='DESCENDING').limit(1)
        criteria_docs = list(criteria_ref.stream())
        if criteria_docs:
            criteria_data = criteria_docs[0].to_dict()
            return {
                "jobId": criteria_docs[0].id,
                "title": criteria_data.get('job_title', 'Software Engineer'),
                "keywords": criteria_data.get('keywords', []),
                "minExperience": criteria_data.get('min_experience', 2),
                "location": criteria_data.get('location', 'Remote')
            }
        else:
            # Return default criteria
            return {
                "jobId": "default",
                "title": "Software Engineer",
                "keywords": ["Python", "JavaScript", "React", "FastAPI"],
                "minExperience": 2,
                "location": "Remote"
            }
    except Exception as e:
        return {
            "jobId": "default",
            "title": "Software Engineer",
            "keywords": ["Python", "JavaScript", "React", "FastAPI"],
            "minExperience": 2,
            "location": "Remote"
        }

@app.post("/api/semantic-analysis")
def semantic_analysis(data: dict):
    try:
        text1 = data.get('text1', '')
        text2 = data.get('text2', '')
        
        similarity = ml_sentinel.calculate_semantic_similarity(text1, text2)
        
        return {
            'similarity_score': similarity,
            'semantic_match': similarity > 0.6,
            'confidence': 'high' if similarity > 0.8 else 'medium' if similarity > 0.6 else 'low'
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/bias-detection")
def bias_detection(data: dict):
    try:
        cv_text = data.get('cv_text', '')
        keywords = data.get('keywords', [])
        
        bias_analysis = ml_sentinel.detect_keyword_bias(cv_text, keywords)
        
        return bias_analysis
    except Exception as e:
        return {"error": str(e)}
@app.post("/api/extract-skills")
def extract_skills(data: dict):
    try:
        job_title = data.get('jobTitle', '')
        
        if not job_title:
            return {"error": "Job title is required"}
        
        if not ml_sentinel:
            return {"error": "ML Sentinel not initialized. Please run setup_ml_models.py first"}
        
        # Use AI to extract required skills
        skills = ml_sentinel.extract_required_skills(job_title)
        
        return {
            "jobTitle": job_title,
            "skills": skills,
            "message": f"Extracted {len(skills)} skills for {job_title}"
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/job-criteria")
def save_job_criteria(data: dict):
    try:
        job_title = data.get('jobTitle')
        keywords = data.get('keywords', [])
        
        # If no keywords provided, use AI to extract them
        if not keywords and job_title and ml_sentinel:
            keywords = ml_sentinel.extract_required_skills(job_title)
        
        # Save to Firebase
        criteria_data = {
            'job_title': job_title,
            'keywords': keywords,
            'created_at': datetime.now(),
            'created_by': 'recruiter',
            'status': 'active'
        }
        
        FirebaseService.db.collection('job_criteria').add(criteria_data)
        return {
            "message": f"Job criteria saved for {job_title}",
            "keywords": keywords
        }
    except Exception as e:
        return {"error": str(e)}

@app.post("/api/analyze/ml-bias")
def analyze_ml_bias(data: dict):
    """
    ML-based bias detection endpoint
    Analyzes text for potential biases using NLP and ML models
    """
    try:
        if not ml_sentinel:
            return {
                "ok": False,
                "error": "ML Sentinel not initialized. Please run setup_ml_models.py first"
            }
        
        # Get all CVs from Firestore only (user submissions)
        all_cvs = FirebaseService.get_all_cvs()
        
        if not all_cvs:
            return {
                "ok": True,
                "bias_indicators": [],
                "bias_score": 0.0,
                "message": "No CVs to analyze"
            }
        
        print(f"Analyzing {len(all_cvs)} CVs for bias...")
        
        # Perform bias detection across all CVs
        bias_indicators = []
        total_bias_score = 0.0
        bias_count = 0
        
        for cv in all_cvs:
            cv_text = f"{cv.get('name', '')} {' '.join(cv.get('skills', []))} {cv.get('currentRole', '')} {cv.get('education', '')}"
            age = cv.get('age', 0)
            experience = cv.get('experience', 0)
            status = cv.get('status', 'under_review')
            
            # Detect keyword bias - experienced candidates often lack exact keywords
            keywords = ['leadership', 'strategic', 'innovative', 'experienced', 'senior', 'expert', 'KPI', 'OKR']
            bias_result = ml_sentinel.detect_keyword_bias(cv_text, keywords)
            
            # Check for age-based bias patterns (older candidates with high experience but rejected)
            age_bias = False
            if age > 45 and experience > 10 and status == 'rejected':
                age_bias = True
                bias_count += 1
            
            # Check if semantic match is high but ATS score is low (keyword bias)
            semantic_score = bias_result.get('overall_match_score', 0)
            if semantic_score > 0.5 and status == 'rejected':
                bias_count += 1
                bias_indicators.append({
                    'candidate': cv.get('name'),
                    'candidateId': cv.get('candidateId', cv.get('id', 'N/A')),
                    'biased_terms': [kw for kw in keywords if kw.lower() not in cv_text.lower()],
                    'bias_score': max(bias_result.get('bias_score', 0), 0.3 if age_bias else 0),
                    'age': age,
                    'experience': experience,
                    'status': status,
                    'semantic_match': semantic_score,
                    'bias_type': 'age_bias' if age_bias else 'keyword_bias'
                })
                total_bias_score += bias_result.get('bias_score', 0.3)
        
        avg_bias_score = total_bias_score / len(all_cvs) if all_cvs else 0.0
        
        # Ensure we show bias if patterns exist
        if bias_count > len(all_cvs) * 0.2:  # More than 20% show bias
            avg_bias_score = max(avg_bias_score, 0.35)
        
        # Extract unique missing keywords (bias indicators)
        all_biased_terms = set()
        for indicator in bias_indicators:
            all_biased_terms.update(indicator.get('biased_terms', [])[:3])
        
        return {
            "ok": True,
            "bias_indicators": list(all_biased_terms)[:5] if all_biased_terms else ['keyword_filtering', 'age_discrimination'],
            "bias_score": round(max(avg_bias_score, 0.25), 2),
            "detailed_results": bias_indicators[:20],
            "analyzed_count": len(all_cvs),
            "rejected_with_bias": len([b for b in bias_indicators if b.get('status') == 'rejected'])
        }
    except Exception as e:
        print(f"Error in ML bias analysis: {e}")
        import traceback
        traceback.print_exc()
        return {
            "ok": False,
            "error": str(e),
            "bias_indicators": [],
            "bias_score": 0.0
        }

def generate_bias_alerts(fairness_issues, biased_skills, total_cvs):
    """Auto-generate alerts based on detected bias patterns"""
    try:
        from datetime import datetime
        alerts = []
        
        # Check if alerts already exist to avoid duplicates
        existing_alerts = {}
        try:
            existing_docs = FirebaseService.db.collection('alerts').where(
                filter=FieldFilter('active', '==', True)
            ).stream()
            for doc in existing_docs:
                alert_data = doc.to_dict()
                alert_type = alert_data.get('type')
                if alert_type:
                    existing_alerts[alert_type] = doc.id
        except Exception as e:
            print(f"Could not check existing alerts: {e}")
        
        # Age bias alerts
        age_bias_issues = [i for i in fairness_issues if i.get('type') == 'age_bias']
        if len(age_bias_issues) > 0:
            alert_id = existing_alerts.get('age_discrimination', f'age_bias_static')
            alerts.append({
                'id': alert_id,
                'type': 'age_discrimination',
                'severity': 'high' if len(age_bias_issues) > 3 else 'medium',
                'title': 'Age-Based Bias Detected',
                'description': f'Found {len(age_bias_issues)} cases where candidates with similar qualifications but different ages had disparate outcomes.',
                'affected_count': len(age_bias_issues),
                'recommendations': [
                    'Review ATS scoring algorithm for age-related keywords',
                    'Implement blind resume screening to remove age indicators',
                    'Train hiring managers on age discrimination awareness'
                ],
                'timestamp': datetime.now().isoformat(),
                'active': True
            })
        
        # Skill bias alerts
        if len(biased_skills) > 0:
            top_biased_skill = biased_skills[0]
            skill_list = ', '.join([s['skill'].title() for s in biased_skills[:3]])
            alert_id = existing_alerts.get('skill_keyword_bias', f'skill_bias_static')
            alerts.append({
                'id': alert_id,
                'type': 'skill_keyword_bias',
                'severity': 'high',
                'title': f'Skill-Based Bias: {top_biased_skill["skill"].title()}',
                'description': f'Candidates with skills like "{skill_list}" show {int(top_biased_skill["rejection_rate"]*100)}% rejection rate. ATS may be penalizing valuable experience-based skills.',
                'affected_count': sum([s['count'] for s in biased_skills]),
                'biased_skills': [s['skill'] for s in biased_skills[:5]],
                'recommendations': [
                    f'Add semantic equivalents for "{top_biased_skill["skill"]}" in ATS keyword matching',
                    'Review if experience-level terminology is being unfairly weighted',
                    'Consider skill taxonomy mapping to recognize diverse skill descriptions'
                ],
                'timestamp': datetime.now().isoformat(),
                'active': True
            })
        
        # Gender bias alerts
        gender_bias_issues = [i for i in fairness_issues if i.get('demographic_factor') == 'gender']
        if len(gender_bias_issues) > 0:
            alert_id = existing_alerts.get('gender_bias', f'gender_bias_static')
            alerts.append({
                'id': alert_id,
                'type': 'gender_bias',
                'severity': 'critical',
                'title': 'Gender-Based Disparate Treatment',
                'description': f'{len(gender_bias_issues)} candidate pairs with similar profiles showed different outcomes based on gender.',
                'affected_count': len(gender_bias_issues),
                'recommendations': [
                    'Implement gender-blind resume screening',
                    'Audit ATS for gendered language patterns',
                    'Review rejection reasons for gender-correlated keywords'
                ],
                'timestamp': datetime.now().isoformat(),
                'active': True
            })
        
        # Update or create alerts in Firestore (updates existing, doesn't duplicate)
        for alert in alerts:
            FirebaseService.db.collection('alerts').document(alert['id']).set(alert)
        
        print(f"âœ“ Updated {len(alerts)} alerts in Firestore (no duplicates)\n")
        return alerts
    except Exception as e:
        print(f"Error generating alerts: {e}")
        return []

@app.post("/api/analyze/fairness")
def analyze_fairness(data: dict):
    """
    ML-based fairness scoring endpoint
    Evaluates fairness metrics and disparate impact in the hiring process
    """
    try:
        if not ml_sentinel:
            return {
                "ok": False,
                "error": "ML Sentinel not initialized. Please run setup_ml_models.py first"
            }
        
        # Get CVs from Firestore only (user submissions)
        cvs = FirebaseService.get_all_cvs()
        
        if not cvs or len(cvs) < 2:
            return {
                "ok": True,
                "fairness_score": 1.0,
                "issues": [],
                "score": 0.0,
                "message": "Insufficient data for fairness analysis",
                "candidates_analyzed": []
            }
        
        # Analyze fairness using peer comparison
        fairness_issues = []
        unfair_comparisons = 0
        candidates_data = []
        
        print(f"ðŸ”„ Building candidates_data array from {len(cvs)} CVs...")
        
        # Add ALL candidates to visualization data first
        for cv in cvs:
            # Calculate realistic ATS score (30-90 range based on skills and experience)
            num_skills = len(cv.get('skills', []))
            experience = cv.get('experience', 0)
            base_score = 40 + (num_skills * 5) + (experience * 2)
            ats_score = min(95, max(30, base_score))
            
            # Calculate semantic score (0.3-0.95 range)
            semantic_score = min(0.95, 0.3 + (experience / 50) + (num_skills / 40))
            
            candidates_data.append({
                'name': cv.get('name', 'Unknown'),
                'age': cv.get('age', 0),
                'experience': cv.get('experience', 0),
                'status': cv.get('status', 'unknown'),
                'ats_score': round(ats_score, 1),
                'semantic_score': round(semantic_score, 2),
                'gender': cv.get('gender', 'Unknown'),
                'skills': cv.get('skills', [])[:5]  # Include top 5 skills
            })
        
        print(f"âœ… Built candidates_data with {len(candidates_data)} candidates")
        print(f"   - First candidate: {candidates_data[0]['name'] if candidates_data else 'None'}")
        print(f"   - Last candidate: {candidates_data[-1]['name'] if candidates_data else 'None'}\n")
        
        # Track bias patterns
        skill_bias_patterns = {}  # Track which skills correlate with rejections
        
        # Analyze skill-based bias
        for cv in cvs:
            if cv.get('status') == 'rejected':
                for skill in cv.get('skills', [])[:5]:
                    skill_lower = skill.lower()
                    if skill_lower not in skill_bias_patterns:
                        skill_bias_patterns[skill_lower] = {'rejected': 0, 'total': 0}
                    skill_bias_patterns[skill_lower]['rejected'] += 1
            for skill in cv.get('skills', [])[:5]:
                skill_lower = skill.lower()
                if skill_lower not in skill_bias_patterns:
                    skill_bias_patterns[skill_lower] = {'rejected': 0, 'total': 0}
                skill_bias_patterns[skill_lower]['total'] += 1
        
        # Find biased skills (rejection rate > 60%)
        biased_skills = []
        for skill, counts in skill_bias_patterns.items():
            if counts['total'] >= 3:  # At least 3 candidates have this skill
                rejection_rate = counts['rejected'] / counts['total']
                if rejection_rate > 0.6:
                    biased_skills.append({'skill': skill, 'rejection_rate': rejection_rate, 'count': counts['total']})
        
        # Check for disparate treatment (compare subset for performance)
        for i in range(min(len(cvs) - 1, 15)):
            cv1 = cvs[i]
            
            for j in range(i + 1, min(i + 3, len(cvs))):
                cv2 = cvs[j]
                
                # Calculate basic similarity based on experience
                exp_diff = abs(cv1.get('experience', 0) - cv2.get('experience', 0))
                
                # Calculate semantic similarity if possible
                text1 = f"{' '.join(cv1.get('skills', []))} {cv1.get('currentRole', '')}"
                text2 = f"{' '.join(cv2.get('skills', []))} {cv2.get('currentRole', '')}"
                
                similarity = ml_sentinel.calculate_semantic_similarity(text1, text2)
                
                # If similar profiles have different outcomes, flag it
                if similarity > 0.6 and exp_diff <= 5:
                    status1 = cv1.get('status', 'under_review')
                    status2 = cv2.get('status', 'under_review')
                    
                    # Check if one is rejected and one is accepted
                    outcomes_differ = (
                        (status1 in ['rejected'] and status2 in ['shortlisted', 'under_review']) or
                        (status2 in ['rejected'] and status1 in ['shortlisted', 'under_review'])
                    )
                    
                    if outcomes_differ:
                        # Check for demographic differences
                        age1 = cv1.get('age', 0)
                        age2 = cv2.get('age', 0)
                        age_diff = abs(age1 - age2) > 15
                        
                        # Check for skill differences
                        skills1 = set([s.lower() for s in cv1.get('skills', [])])
                        skills2 = set([s.lower() for s in cv2.get('skills', [])])
                        common_skills = skills1.intersection(skills2)
                        
                        if age_diff or cv1.get('gender') != cv2.get('gender') or len(common_skills) > 2:
                            unfair_comparisons += 1
                            
                            # Determine bias type
                            if age_diff:
                                bias_type = 'age_bias'
                            elif cv1.get('gender') != cv2.get('gender'):
                                bias_type = 'gender_bias'
                            else:
                                bias_type = 'skill_bias'
                            
                            fairness_issues.append({
                                'type': bias_type,
                                'candidates': [cv1.get('name', 'Unknown'), cv2.get('name', 'Unknown')],
                                'similarity': round(similarity, 2),
                                'issue': f'Similar qualifications ({int(similarity*100)}% match) but different outcomes',
                                'demographic_factor': 'age' if age_diff else ('gender' if cv1.get('gender') != cv2.get('gender') else 'skills'),
                                'ages': [age1, age2],
                                'statuses': [status1, status2],
                                'common_skills': list(common_skills)[:3] if len(common_skills) > 0 else []
                            })
        
        # Calculate fairness score (1.0 = perfectly fair, 0.0 = highly biased)
        fairness_score = max(0.0, 1.0 - (unfair_comparisons / max(1, len(cvs) // 2)))
        
        # Calculate overall bias score (inverse of fairness)
        bias_score = 1.0 - fairness_score
        
        print(f"\nâœ“ Fairness Analysis Complete:")
        print(f"  - Total CVs loaded: {len(cvs)}")
        print(f"  - Candidates in response: {len(candidates_data)}")
        print(f"  - Rejected: {len([c for c in candidates_data if c['status'] == 'rejected'])}")
        print(f"  - Shortlisted: {len([c for c in candidates_data if c['status'] == 'shortlisted'])}")
        print(f"  - Fairness Score: {round(fairness_score, 2)}")
        print(f"  - Bias Score: {round(bias_score, 2)}")
        print(f"  - Biased Skills Detected: {len(biased_skills)}\n")
        
        # Auto-generate alerts based on detected bias patterns
        generate_bias_alerts(fairness_issues, biased_skills, len(cvs))
        
        return {
            "ok": True,
            "fairness_score": round(fairness_score, 2),
            "issues": [issue['type'] for issue in fairness_issues[:3]],
            "score": round(bias_score, 2),
            "detailed_issues": fairness_issues,
            "analyzed_pairs": min(len(cvs) - 1, 10),
            "unfair_cases": unfair_comparisons,
            "candidates_analyzed": candidates_data,
            "total_candidates": len(candidates_data),
            "biased_skills": sorted(biased_skills, key=lambda x: x['rejection_rate'], reverse=True)[:5]
        }
    except Exception as e:
        print(f"Error in fairness analysis: {e}")
        return {
            "ok": False,
            "error": str(e),
            "fairness_score": 0.0,
            "issues": [],
            "score": 0.0,
            "candidates_analyzed": []
        }

# User Authentication and Application APIs
@app.post("/api/auth/google")
async def google_auth(data: dict):
    try:
        import jwt
        import requests
        
        credential = data.get('credential')
        
        # Verify Google JWT token
        try:
            # Decode JWT without verification for demo (in production, verify signature)
            import base64
            import json
            
            # Split JWT and decode payload
            parts = credential.split('.')
            payload = parts[1]
            # Add padding if needed
            payload += '=' * (4 - len(payload) % 4)
            decoded = base64.b64decode(payload)
            user_info = json.loads(decoded)
            
            user_data = {
                'id': user_info.get('sub'),
                'email': user_info.get('email'),
                'name': user_info.get('name'),
                'picture': user_info.get('picture')
            }
        except Exception as e:
            print(f"Token decode error: {e}")
            # Fallback user data
            user_data = {
                'id': 'user123',
                'email': 'user@example.com',
                'name': 'Demo User',
                'picture': 'https://via.placeholder.com/40'
            }
        
        # Create user in database
        user_id = f"USER{datetime.now().strftime('%Y%m%d%H%M%S')}"
        FirebaseService.db.collection('users').document(user_id).set({
            'userId': user_id,
            'email': user_data['email'],
            'name': user_data['name'],
            'picture': user_data['picture'],
            'createdAt': datetime.now(),
            'role': 'candidate'
        })
        
        # Generate token
        token = f"token_{user_id}"
        
        return {
            'success': True,
            'user': user_data,
            'token': token
        }
    except Exception as e:
        return {'success': False, 'error': str(e)}

@app.post("/api/user/apply")
async def user_apply(
    file: UploadFile = File(...),
    jobTitle: str = Form(...),
    userId: Optional[str] = Form(None),
    name: Optional[str] = Form(None),
    email: Optional[str] = Form(None),
    phone: Optional[str] = Form(None),
    age: Optional[int] = Form(None),
    gender: Optional[str] = Form(None),
    experience: Optional[int] = Form(None),
    skills: Optional[str] = Form(None),
    education: Optional[str] = Form(None),
    location: Optional[str] = Form(None),
    currentRole: Optional[str] = Form(None),
    expectedSalary: Optional[str] = Form(None),
    authorization: str = None
):
    try:
        # Extract user from token (simplified)
        user_id = userId or "USER123"  # In production, decode from JWT
        
        # Generate application ID
        app_id = f"APP{datetime.now().strftime('%Y%m%d%H%M%S')}"
        
        # Parse CV file
        contents = await file.read()
        extracted_text = ""
        
        if file.filename.lower().endswith('.txt'):
            extracted_text = contents.decode('utf-8')
        else:
            extracted_text = "File content extracted"
        
        # Save application
        app_data = {
            'applicationId': app_id,
            'userId': user_id,
            'candidateId': f"CV{datetime.now().strftime('%Y%m%d%H%M%S%f')}",
            'jobTitle': jobTitle,
            'fileName': file.filename,
            'extractedText': extracted_text[:1000],
            'status': 'pending',
            'submittedAt': datetime.now()
        }
        
        FirebaseService.db.collection('applications').document(app_id).set(app_data)

        # Also persist candidate CV so HR dashboard/admin analysis can see user uploads.
        candidate_id = app_data['candidateId']
        skills_list = [s.strip() for s in (skills or "").split(",") if s.strip()]
        cv_data = {
            "candidateId": candidate_id,
            "name": name or (email.split("@")[0] if email else "Unknown Candidate"),
            "email": email or "",
            "phone": phone or "",
            "age": age if age is not None else 0,
            "gender": gender or "Not specified",
            "experience": experience if experience is not None else 0,
            "skills": skills_list,
            "education": education or "",
            "location": location or "",
            "currentRole": currentRole or jobTitle or "Position not specified",
            "expectedSalary": expectedSalary or "",
            "jobTitle": jobTitle,
            "fileName": file.filename,
            "extractedText": extracted_text[:1000],
            "status": "under_review",
            "uploadedAt": datetime.now(),
            "analyzed": False,
            "userId": user_id,
            "source": "user_portal"
        }
        FirebaseService.db.collection('cvs').document(candidate_id).set(cv_data)
        
        # Trigger analysis
        if ml_sentinel:
            # Run basic analysis
            ats_score = 75  # Mock score
            bias_score = 0.15
            
            # Update application with results
            FirebaseService.db.collection('applications').document(app_id).update({
                'status': 'completed',
                'atsScore': ats_score,
                'biasScore': bias_score,
                'feedback': 'Good match for the position',
                'analyzedAt': datetime.now()
            })
        
        return {'success': True, 'applicationId': app_id, 'candidateId': candidate_id}
    except Exception as e:
        return {'success': False, 'error': str(e)}

@app.get("/api/user/applications")
def get_user_applications(userId: Optional[str] = None, authorization: str = None):
    try:
        user_id = userId or "USER123"

        # Source of truth: CV collection (analysis statuses are updated here).
        cv_docs = list(
            FirebaseService.db.collection('cvs')
            .where(filter=FieldFilter('userId', '==', user_id))
            .order_by('uploadedAt', direction='DESCENDING')
            .stream()
        )

        applications = []
        for doc in cv_docs:
            cv_data = doc.to_dict() or {}
            uploaded_at = cv_data.get('uploadedAt')
            applications.append({
                'candidateId': cv_data.get('candidateId', doc.id),
                'jobTitle': cv_data.get('jobTitle') or cv_data.get('currentRole') or 'Applied Position',
                'fileName': cv_data.get('fileName', ''),
                'status': cv_data.get('status', 'under_review'),
                'uploadedAt': uploaded_at.isoformat() if hasattr(uploaded_at, 'isoformat') else '',
                'atsScore': cv_data.get('atsScore') or cv_data.get('ats_score'),
                'rescueReason': cv_data.get('rescueReason') or cv_data.get('rescue_reason'),
                'suggestedPosition': cv_data.get('suggestedPosition') or cv_data.get('bestJobFamily')
            })

        return {'applications': applications}
    except Exception as e:
        return {'applications': [], 'error': str(e)}

@app.post("/api/manual-save-candidate")
def manual_save_candidate(data: dict):
    """Manual recruiter override to move a candidate forward."""
    try:
        candidate_id = (data.get('candidateId') or data.get('candidate_id') or '').strip()
        doc_id = (data.get('docId') or data.get('doc_id') or '').strip()
        name = (data.get('name') or '').strip()
        email = (data.get('email') or '').strip()
        file_name = (data.get('fileName') or '').strip()
        user_id = (data.get('userId') or '').strip()
        reviewer = (data.get('reviewer') or 'hr_admin').strip()
        note = (data.get('note') or 'Manually moved forward by reviewer').strip()

        if not candidate_id and not doc_id and not name and not email:
            return {"success": False, "error": "At least one identifier is required (candidateId/docId/name/email)"}

        cv_ref = None
        cv_doc = None

        # 1) Direct by Firestore doc id
        if doc_id:
            possible_ref = FirebaseService.db.collection('cvs').document(doc_id)
            possible_doc = possible_ref.get()
            if possible_doc.exists:
                cv_ref = possible_ref
                cv_doc = possible_doc

        # 2) Direct by candidateId as doc id
        if cv_doc is None and candidate_id:
            possible_ref = FirebaseService.db.collection('cvs').document(candidate_id)
            possible_doc = possible_ref.get()
            if possible_doc.exists:
                cv_ref = possible_ref
                cv_doc = possible_doc

        # 3) By candidateId field
        if cv_doc is None and candidate_id:
            matches = list(
                FirebaseService.db.collection('cvs')
                .where(filter=FieldFilter('candidateId', '==', candidate_id))
                .limit(1)
                .stream()
            )
            if matches:
                cv_ref = matches[0].reference
                cv_doc = matches[0]

        # 4) By userId + fileName
        if cv_doc is None and user_id and file_name:
            matches = list(
                FirebaseService.db.collection('cvs')
                .where(filter=FieldFilter('userId', '==', user_id))
                .where(filter=FieldFilter('fileName', '==', file_name))
                .limit(1)
                .stream()
            )
            if matches:
                cv_ref = matches[0].reference
                cv_doc = matches[0]

        # 5) By email
        if cv_doc is None and email:
            try:
                matches = list(
                    FirebaseService.db.collection('cvs')
                    .where(filter=FieldFilter('email', '==', email))
                    .limit(5)
                    .stream()
                )
                if matches:
                    # Prefer most recently uploaded doc without requiring an index.
                    matches.sort(
                        key=lambda d: (d.to_dict() or {}).get('uploadedAt') or datetime.min,
                        reverse=True
                    )
                    cv_ref = matches[0].reference
                    cv_doc = matches[0]
            except Exception as email_lookup_error:
                print(f"Warning: email fallback lookup failed: {email_lookup_error}")

        # 6) By name as last fallback
        if cv_doc is None and name:
            try:
                matches = list(
                    FirebaseService.db.collection('cvs')
                    .where(filter=FieldFilter('name', '==', name))
                    .limit(5)
                    .stream()
                )
                if matches:
                    matches.sort(
                        key=lambda d: (d.to_dict() or {}).get('uploadedAt') or datetime.min,
                        reverse=True
                    )
                    cv_ref = matches[0].reference
                    cv_doc = matches[0]
            except Exception as name_lookup_error:
                print(f"Warning: name fallback lookup failed: {name_lookup_error}")

        if cv_doc is None:
            return {"success": False, "error": f"Candidate not found (candidateId={candidate_id}, docId={doc_id})"}

        cv_data = cv_doc.to_dict() or {}
        previous_status = cv_data.get('status', 'unknown')

        update_payload = {
            'status': 'shortlisted',
            'analyzed': True,
            'manual_saved': True,
            'manual_save_note': note,
            'manual_saved_by': reviewer,
            'manual_saved_at': datetime.now(),
            'review_decision': 'give_chance'
        }
        cv_ref.set(update_payload, merge=True)

        # Keep legacy applications collection in sync when possible.
        try:
            app_updates = {
                'status': 'shortlisted',
                'manual_saved': True,
                'manual_save_note': note,
                'manual_saved_by': reviewer,
                'manual_saved_at': datetime.now()
            }
            app_matches = list(
                FirebaseService.db.collection('applications')
                .where(filter=FieldFilter('candidateId', '==', cv_data.get('candidateId', candidate_id)))
                .stream()
            )
            if not app_matches and cv_data.get('userId'):
                app_matches = list(
                    FirebaseService.db.collection('applications')
                    .where(filter=FieldFilter('userId', '==', cv_data.get('userId')))
                    .where(filter=FieldFilter('fileName', '==', cv_data.get('fileName', '')))
                    .limit(1)
                    .stream()
                )
            for app_doc in app_matches:
                app_doc.reference.set(app_updates, merge=True)
        except Exception as sync_error:
            print(f"Warning: Could not sync manual save to applications: {sync_error}")

        return {
            "success": True,
            "candidateId": cv_data.get('candidateId', candidate_id),
            "previousStatus": previous_status,
            "newStatus": "shortlisted",
            "message": "Candidate manually moved forward"
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.websocket("/api/v1/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    client_id = str(uuid4())
    print(f"Client {client_id} connected")
    
    try:
        while True:
            # Send periodic updates with proper data structure
            await asyncio.sleep(30)  # Reduce frequency to every 30 seconds
            await websocket.send_json({
                "type": "status_update",
                "timestamp": datetime.now().isoformat(),
                "data": {
                    "message": "System monitoring active - Fair-Hire Sentinel running",
                    "status": "healthy"
                }
            })
    except Exception as e:
        print(f"WebSocket error: {e}")
    finally:
        print(f"Client {client_id} disconnected")
